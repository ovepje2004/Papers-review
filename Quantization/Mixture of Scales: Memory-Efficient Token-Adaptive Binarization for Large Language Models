## Abstract

- Binarization(-1, 1)은 LLM의 크기를 효과적으로 줄임, 그러나 언어적 성능 크게 저하
- 새로운 이진화 기법인 Mixture of scales(BinaryMoS) 제안.
    - 이진 가중치에 대해 여러개의 scaling experts 활용
    - 각 token마다 experts를 통해 scaling factors 생성
- 이와 같은 token-adaptive 방식은 문맥에 따라 이진 가중치 값을 조절할 수 있어 표현력 강화
- Adaptation process는 전체(x), scaling factors(o)에만 적용하여 기존 방식과 유사한 수준의 압축 효율

## 1. Intro

- 기존 양자화 기법인 GPTQ, AWQ, OWQ 등은 16bit floating point weights를 4비트 표현으로 변환 성공
- 나아가 Binarization은 가중치를 1비트로 줄여 모델 크기를 1/16 크기로 압축 가능
    - 근데 aggressive compression → 가중치의 representational capacity 제한
    - LLM 특화 Binarization 연구 중이지만 다음과 같은 문제 존재.
        - high memory overhead
        - struggle to achieve sufficient accuracy

## 2. Background

- **2-1. Binarization of LLMs**
    - 기본적으로 Full-Precision 가중치 파라미터를 1비트 값으로 변환하는 방식
        - full-precision 가중치 행렬 W_FP에서 W_FP의 평균을 뺀 값을 Sign함수로  binarization.
            - Sign(x)는 부호함수로, 양수면 1, 음수면 -1, 0이면 0으로 표현
            - W_B는  binarization된 가중치 행렬
        
        $$
        W_B = \alpha·Sign(W_{FP}-\overline{W}_{FP})
        $$
        
        - α는 출력 차원 크기의 Scaling factors이고,  binarization된 가중치 값을 조정
            - 출력 채널 별(=행 단위)로 가중치 세기 조절
            - 기존 가중치와 binarization 거친 가중치의 표현 gap을 보완
            
    - 근데 전통적인 딥러닝 모델에서는 성공적인데, LLM에서는 성능이 크게 저하. 그래서 여러 기법 개발ing
        - PB-LLM(Partial Binarization): 일부만 이진화하고 중요한(salient) 가중치를  high-precision로 유지
        - BiLLM: Scaling factors를 더 정교하게 할당.
            - 가중치가 정규분포를 따른다고 가정하고, 평균에 가까운 concentrated 가중치와 평균에서 먼 sparse 가중치로 구분. 각 그룹에 대해 서로 다른 Scaling factors 할당하여 오차 최소화.
        - One-bit: 입력/출력 양방향에 Scaling factors 적용하여 정확도 향상(기존엔 출력 차원만 적용)
            - 이중차원 스케일링은 이진화 오차를 양방향으로 줄임
            
            $$
            Y = [(X⊙S_{in})Sign(W^T_{FP})]⊙S_{out}
            $$
            
            - 입력 X에 대해 S_in 곱해서 스케일 조정한 후 이진화된 가중치 행렬과 행렬곱
            - 이후 출력값 각각에 대해 S_out 곱해서 스케일 조정
    
    그치만 여전히 full-precision 모델과 binarized 모델 간의 성능 격차 존재
    
    ⇒ 메모리 효율성은 유지하면서 정확도 격차를 줄여야 함.
